# -*- coding: utf-8 -*-
"""Task_II_Leaf_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16O8XMlr6wf45CMmo4jU_mRaa_P2Wjmy6

# Task II : Leaf Detection
Group members :    
- 1239686 - Dana Putra Kembara
- 1311085 - Efraim Partogi Nahotasi

In this code we will develop a CNN for object detection for plant leafs using part of the Leaf Segmentation Challenge (LSC) datasets.

#Pre-processing

In pre-processing we compile libraries and function that will be used.

## Import Libraries

Using libraries that will be used on the project.
"""

!pip install d2l==0.16.2
!pip install opencv-python
!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from d2l import torch as d2l

import numpy as np
import matplotlib.pyplot as plt
import random
import cv2
import glob
import os
import torch
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms

from torch import nn
from torch.nn import functional as F
from torchvision.transforms import ToPILImage
from torchvision.ops.boxes import box_area

from PIL import Image
from PIL import ImageDraw

from collections import Counter
from scipy.optimize import linear_sum_assignment
from mpl_toolkits.axes_grid1 import ImageGrid

from sklearn.model_selection import train_test_split

from torchsummary import summary

"""## Function that will be used

This function from external files in practical section that will be used.
"""

def box_iou(boxes1, boxes2):
    """
    Compute the Intersection over Union (IoU) between two sets of bounding boxes. The format of the bounding boxes
    should be in (x1, y1, x2, y2) format.

    Args:
        boxes1 (torch.Tensor): A tensor of shape (N, 4) in (x1, y1, x2, y2) format.
        boxes2 (torch.Tensor): A tensor of shape (M, 4) in (x1, y1, x2, y2) format.

    Returns:
        tuple[torch.Tensor, torch.Tensor]: A tuple of (iou, union) where:
            iou (torch.Tensor): A tensor of shape (N, M) containing the pairwise IoU values
            between the boxes in boxes1 and boxes2.
            union (torch.Tensor): A tensor of shape (N, M) containing the pairwise union
            areas between the boxes in boxes1 and boxes2.
    """
    # Calculate boxes area
    area1 = box_area(boxes1)  # [N,]
    area2 = box_area(boxes2)  # [M,]

    # Compute the coordinates of the intersection of each pair of bounding boxes
    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]
    # Need clamp(min=0) in case they do not intersect, then we want intersection to be 0
    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]

    # Since the size of the variables is different, pytorch broadcast them
    # area1[:, None] converts size from [N,] to [N,1] to help broadcasting
    union = area1[:, None] + area2 - inter  # [N,M]

    iou = inter / union
    return iou, union


def generalized_box_iou(boxes1, boxes2):
    """
    Computes the generalized box intersection over union (IoU) between two sets of bounding boxes.
    The IoU is defined as the area of overlap between the two bounding boxes divided by the area of union.

    Args:
        boxes1: A tensor containing the coordinates of the bounding boxes for the first set.
            Shape: [batch_size, num_boxes, 4]
        boxes2: A tensor containing the coordinates of the bounding boxes for the second set.
            Shape: [batch_size, num_boxes, 4]

    Returns:
        A tensor containing the generalized IoU between `boxes1` and `boxes2`.
            Shape: [batch_size, num_boxes1, num_boxes2]
    """
    # Check for degenerate boxes that give Inf/NaN results
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()

    # Calculate the IoU and union of each pair of bounding boxes
    iou, union = box_iou(boxes1, boxes2)

    # Compute the coordinates of the intersection of each pair of bounding boxes
    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])
    wh = (rb - lt).clamp(min=0)  # [N,M,2]

    # Compute the area of the bounding box that encloses both input boxes
    C = wh[:, :, 0] * wh[:, :, 1]

    return iou - (C - union) / C


class HungarianMatcher(nn.Module):
    """This class computes an assignment between the targets and the predictions of the network

    For efficiency reasons, the targets don't include the no_object. Because of this, in general,
    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,
    while the others are un-matched (and thus treated as non-objects).
    """

    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):
        """Creates the matcher

        Params:
            cost_class: This is the relative weight of the classification error in the matching cost
            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost
            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost
        """
        super().__init__()
        self.cost_class = cost_class
        self.cost_bbox = cost_bbox
        self.cost_giou = cost_giou
        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, "all costs cant be 0"

    @torch.no_grad()
    def forward(self, outputs, targets):
        """Performs the matching

        Params:
            outputs: This is a dict that contains at least these entries:
                 "pred_logits": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits
                 "pred_boxes": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates

            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:
                 "labels": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth
                           objects in the target) containing the class labels
                 "boxes": Tensor of dim [num_target_boxes, 4] containing the target box coordinates

        Returns:
            A list of size batch_size, containing tuples of (index_i, index_j) where:
                - index_i is the indices of the selected predictions (in order)
                - index_j is the indices of the corresponding selected targets (in order)
            For each batch element, it holds:
                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)
        """
        bs, num_queries = outputs["pred_logits"].shape[:2]

        # We flatten to compute the cost matrices in a batch
        out_prob = outputs["pred_logits"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]
        out_bbox = outputs["pred_boxes"].flatten(0, 1)  # [batch_size * num_queries, 4]

        # Also concat the target labels and boxes
        tgt_ids = torch.cat([v["labels"] for v in targets])
        tgt_bbox = torch.cat([v["boxes"] for v in targets])

        # Compute the classification cost. Contrary to the loss, we don't use the NLL,
        # but approximate it in 1 - proba[target class].
        # The 1 is a constant that doesn't change the matching, it can be ommitted.
        cost_class = -out_prob[:, tgt_ids]

        # Compute the L1 cost between boxes
        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)

        # Compute the giou cost betwen boxes
        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))

        # Final cost matrix
        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou
        C = C.view(bs, num_queries, -1).cpu()

        sizes = [len(v["boxes"]) for v in targets]
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [
            (torch.as_tensor(i, dtype=torch.int64).to("cuda"), torch.as_tensor(j, dtype=torch.int64).to("cuda"))
            for i, j in indices
        ]


def get_src_permutation_idx(indices):
    # permute predictions following indices
    batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
    src_idx = torch.cat([src for (src, _) in indices])
    return batch_idx, src_idx


def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, num_classes=20):
    """
    Calculates mean average precision
    Parameters:
        pred_boxes (list): list of lists containing all bboxes with each bboxes
        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]
        true_boxes (list): Similar as pred_boxes except all the correct ones
        iou_threshold (float): threshold where predicted bboxes is correct
        box_format (str): "midpoint" or "corners" used to specify bboxes
        num_classes (int): number of classes
    Returns:
        float: mAP value across all classes given a specific IoU threshold
    """

    if len(pred_boxes) == 0 or len(true_boxes) == 0:
        return 0.0

    # list storing all AP for respective classes
    average_precisions = []

    # used for numerical stability later on
    epsilon = 1e-6

    for c in range(num_classes):
        detections = []
        ground_truths = []

        # Go through all predictions and targets,
        # and only add the ones that belong to the
        # current class c
        for detection in pred_boxes:
            if detection[1] == c:
                detections.append(detection)

        for true_box in true_boxes:
            if true_box[1] == c:
                ground_truths.append(true_box)

        # find the amount of bboxes for each training example
        # Counter here finds how many ground truth bboxes we get
        # for each training example, so let's say img 0 has 3,
        # img 1 has 5 then we will obtain a dictionary with:
        # amount_bboxes = {0:3, 1:5}
        amount_bboxes = Counter([gt[0] for gt in ground_truths])

        # We then go through each key, val in this dictionary
        # and convert to the following (w.r.t same example):
        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}
        for key, val in amount_bboxes.items():
            amount_bboxes[key] = torch.zeros(val)

        # sort by box probabilities which is index 2
        detections.sort(key=lambda x: x[2], reverse=True)
        TP = torch.zeros((len(detections)))
        FP = torch.zeros((len(detections)))
        total_true_bboxes = len(ground_truths)

        # If none exists for this class then we can safely skip
        if total_true_bboxes == 0:
            continue

        for detection_idx, detection in enumerate(detections):
            # Only take out the ground_truths that have the same
            # training idx as detection
            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]

            best_iou = 0

            for idx, gt in enumerate(ground_truth_img):
                iou, _ = box_iou(
                    box_cxcywh_to_xyxy(torch.tensor(detection[3:]).unsqueeze(0)),
                    box_cxcywh_to_xyxy(torch.tensor(gt[3:]).unsqueeze(0)),
                )

                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = idx

            if best_iou > iou_threshold:
                # only detect ground truth detection once
                if amount_bboxes[detection[0]][best_gt_idx] == 0:
                    # true positive and add this bounding box to seen
                    TP[detection_idx] = 1
                    amount_bboxes[detection[0]][best_gt_idx] = 1
                else:
                    FP[detection_idx] = 1

            # if IOU is lower then the detection is a false positive
            else:
                FP[detection_idx] = 1

        TP_cumsum = torch.cumsum(TP, dim=0)
        FP_cumsum = torch.cumsum(FP, dim=0)
        recalls = TP_cumsum / (total_true_bboxes + epsilon)
        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)
        precisions = torch.cat((torch.tensor([1]), precisions))
        recalls = torch.cat((torch.tensor([0]), recalls))
        # torch.trapz for numerical integration
        average_precisions.append(torch.trapz(precisions, recalls))

    return sum(average_precisions) / len(average_precisions)


class APCalculator:
    """A class for calculating average precision (AP).

    This class is built to be used in a training loop, allowing the ground truth (GTs)
    to be initialized once in the __init__ constructor, and then reused many times by calling
    the `calculate_map()` method.

    Attributes:
        iou_threshold (float): The intersection over union (IoU) threshold used for the AP calculation. Defaults to 0.5.
        data_iter (torch.data.Dataloader): A PyTorch dataloader that provides images and targets (e.g., bounding boxes)
            for the dataset.
        n_classes (int): The number of object classes.
        GTs (List[List[Union[int, float]]]): A list of ground truth_target values for each bounding box in the dataset.
        preds (List[List[Union[int, float]]]): A list of predicted_target values for each bounding box in the dataset.

    Args:
        data_iter (torch.data.Dataloader): A PyTorch dataloader that provides images and targets (e.g., bounding boxes)
            for the dataset.
        n_classes (int): The number of object classes.
        iou_threshold (float, optional): The intersection over union (IoU) threshold used for the AP calculation.
            Defaults to 0.5.
    """

    def __init__(self, data_iter, n_classes, iou_threshold=0.5):
        """Initializes the APCalculator object with the specified data iterator, number of classes,
        and IoU threshold."""
        self.iou_threshold = iou_threshold
        self.data_iter = data_iter
        self.n_classes = n_classes
        self.GTs = []

        # Get ground truth target values for each bounding box in the dataset
        for i, (images, targets) in enumerate(self.data_iter):
            new_targets = []
            for idx in range(targets["labels"].shape[0]):
                labels = targets["labels"][idx]
                boxes = targets["boxes"][idx]
                new_targets.append(
                    {
                        "labels": labels[labels != -1].cpu().detach().numpy(),
                        "boxes": boxes[labels != -1].cpu().detach().numpy(),
                    }
                )

            for j in range(images.shape[0]):
                for k in range(new_targets[j]["labels"].shape[0]):
                    label_info = []
                    label_info.append(i * self.data_iter.batch_size + j)  # image index
                    label_info.append(new_targets[j]["labels"][k])  # class label
                    label_info.append(1)  # class label
                    label_info.extend(new_targets[j]["boxes"][k].tolist())  # bounding box coordinates
                    self.GTs.append(label_info)

    def calculate_map(self, net, nms_threshold=0.1):
        """Calculates the mean average precision (mAP) for the given object detection network.

        Args:
            net (torch.nn.Module): The object detection network.
            nms_threshold (float, optional): The non-maximum suppression (NMS) threshold. Defaults to 0.1.

        Returns:
            float: The mean average precision (mAP) for the given network and ground truth targets.
        """
        net.eval()
        preds = []

        for i, (images, targets) in enumerate(self.data_iter):
            device = d2l.try_gpu()
            images = images.to(device)
            outputs = net(images)
            outputs["pred_logits"] = outputs["pred_logits"].cpu()
            outputs["pred_boxes"] = outputs["pred_boxes"].cpu()

            prob = F.softmax(outputs["pred_logits"][0], dim=1)
            top_p, top_class = prob.topk(1, dim=1)
            # top_class = torch.where(top_p > 0.7, top_class, self.n_classes)

            boxes = outputs["pred_boxes"][0][top_class.squeeze() != self.n_classes]
            scores = top_p[top_class != self.n_classes]
            top_class = top_class[top_class != self.n_classes]

            sel_boxes_idx = torchvision.ops.nms(
                boxes=box_cxcywh_to_xyxy(boxes), scores=scores, iou_threshold=nms_threshold
            )

            boxes = boxes[sel_boxes_idx].cpu().detach().numpy()
            scores = scores[sel_boxes_idx].cpu().detach().numpy()
            top_class = top_class[sel_boxes_idx].cpu().detach().numpy()
            for j in range(images.shape[0]):
                for k in range(boxes.shape[0]):
                    pred_info = []
                    pred_info.append(i * self.data_iter.batch_size + j)
                    pred_info.append(top_class[k])
                    pred_info.append(scores[k])
                    pred_info.extend(boxes[k].tolist())
                    preds.append(pred_info)

        self.preds = preds
        return float(
            mean_average_precision(preds, self.GTs, num_classes=self.n_classes, iou_threshold=self.iou_threshold)
        )


def plot_bbox(img, boxes):
    """
    Plot bounding boxes on the given image.

    Bounding boxes are defined as tuples (x, y, w, h), where:
        - (x, y) are the center coordinates of the bounding box
        - w is the width of the bounding box
        - h is the height of the bounding box

    The bounding boxes are drawn in blue with a width of 3 pixels.

    Args:
        img (PIL.Image): The image to plot bounding boxes on.
        boxes (List[Tuple[int, int, int, int]]): A list of bounding boxes.

    Returns:
        A PIL.Image object representing the original image with the bounding boxes plotted on it.
    """
    draw = ImageDraw.Draw(img)
    for box in boxes:
        # print(box)
        x, y, w, h = box
        draw.rectangle(
            (x - w / 2, y - h / 2, x + w / 2, y + h / 2),
            outline="white",
            width=5,
        )
    return img

def plot_grid(imgs, nrows, ncols):
    """
    This function plots a grid of images using the given list of images.
    The grid has the specified number of rows and columns.
    The size of the figure is set to 10x10 inches.
    Returns None.

    Parameters:
        - imgs (List[PIL.Image]): a list of PIL.Image objects to plot
        - nrows (int): the number of rows in the grid
        - ncols (int): the number of columns in the grid

    Returns:
        None.
    """
    assert len(imgs) == nrows * ncols, "nrows*ncols must be equal to the number of images"
    fig = plt.figure(figsize=(10.0, 10.0))
    grid = ImageGrid(
        fig,
        111,  # similar to subplot(111)
        nrows_ncols=(nrows, ncols),
        axes_pad=0.1,  # pad between axes in inch.
    )
    for ax, im in zip(grid, imgs):
        # Iterating over the grid returns the Axes.
        ax.imshow(im)
    plt.show()


def box_cxcywh_to_xyxy(x):
    """
    Convert bounding boxes from (center x, center y, width, height) format to
    (x1, y1, x2, y2) format.

    Args:
        x (torch.Tensor): A tensor of shape (N, 4) in (center x, center y,
            width, height) format.

    Returns:
        torch.Tensor: A tensor of shape (N, 4) in (x1, y1, x2, y2) format.
    """
    x_c, y_c, w, h = x.unbind(-1)
    x1 = x_c - 0.5 * w
    y1 = y_c - 0.5 * h
    x2 = x_c + 0.5 * w
    y2 = y_c + 0.5 * h
    b = torch.stack([x1, y1, x2, y2], dim=-1)
    return b


def box_xyxy_to_cxcywh(x):
    """
    Convert bounding boxes from (x1, y1, x2, y2) format to (center_x, center_y, width, height)
    format.

    Args:
        x (torch.Tensor): A tensor of shape (N, 4) in (x1, y1, x2, y2) format.

    Returns:
        torch.Tensor: A tensor of shape (N, 4) in (center_x, center_y, width, height) format.
    """
    x0, y0, x1, y1 = x.unbind(-1)
    center_x = (x0 + x1) / 2
    center_y = (y0 + y1) / 2
    width = x1 - x0
    height = y1 - y0
    b = torch.stack([center_x, center_y, width, height], dim=-1)
    return b


def box_xywh_to_xyxy(x):
    """
    Convert bounding box from (x, y, w, h) format to (x1, y1, x2, y2) format.

    Args:
        x (torch.Tensor): A tensor of shape (N, 4) in (x, y, w, h) format.

    Returns:
        torch.Tensor: A tensor of shape (N, 4) in (x1, y1, x2, y2) format.
    """
    x_min, y_min, w, h = x.unbind(-1)
    x_max = x_min + w
    y_max = y_min + h
    b = torch.stack([x_min, y_min, x_max, y_max], dim=-1)
    return b


def predict(model, img, n_classes, nms_threshold=0.1):
    model.eval()
    img_size = img.shape[1]
    outputs = model(img.unsqueeze(0).to("cuda"))

    prob = F.softmax(outputs["pred_logits"][0], dim=1)
    top_p, top_class = prob.topk(1, dim=1)

    img = ToPILImage()(img)

    boxes = outputs["pred_boxes"][0][top_class.squeeze() != n_classes]
    scores = top_p[top_class != n_classes]
    top_class = top_class[top_class != n_classes]

    sel_boxes_idx = torchvision.ops.nms(boxes=box_cxcywh_to_xyxy(boxes), scores=scores, iou_threshold=nms_threshold)

    boxes = boxes[sel_boxes_idx].cpu().detach().numpy() * img_size
    scores = scores[sel_boxes_idx].cpu().detach().numpy()
    top_class = top_class[sel_boxes_idx].cpu().detach().numpy()

    return boxes, top_class, scores


class ResizeWithBBox(object):
    """
    Resizes an image and its corresponding bounding boxes.
    """

    def __init__(self, size):
        """
        Initializes the transform.

        Args:
            size: tuple, containing the new size of the image.
        """
        self.size = size

    def __call__(self, image, boxes):
        """
        Applies the transform to an image and its corresponding bounding boxes.

        Args:
            image: PIL.Image object, containing the original image.
            boxes: a list of bounding box coordinates in the format [cx, cy, width, height].

        Returns:
        new_image: PIL.Image object, containing the resized image.
        new_boxes: the bounding box coordinates scaled to the new image size. Range [0-1]
        """

        width_scale = self.size[0] / image.size[0]
        height_scale = self.size[1] / image.size[1]
        new_image = image.resize(self.size)

        new_boxes = []
        for box in boxes:
            x1, y1, w, h = box
            new_x1 = x1 * width_scale
            new_y1 = y1 * height_scale
            new_w = w * width_scale
            new_h = h * height_scale
            new_boxes.append([new_x1 / self.size[0], new_y1 / self.size[1], new_w / self.size[0], new_h / self.size[1]])

        return new_image, new_boxes

"""## Download the dataset
First, let's download the dataset. It consists of images of plant, bounding box annotations, and leaf counts annotations.
"""

!git clone https://git.wur.nl/deep-learning-course/leaf-dataset.git

"""Let's have a look at one image."""

image = Image.open("leaf-dataset/detection/ara2012_plant001_rgb.png")
plt.imshow(image)

image.size

"""# The `Dataset`class
Now, let's create a dataset customized to our data.
We will call it `LeafDetectionDataset`.


**Exercise:** complete the missing parts.
"""

import glob
import os
from sklearn.model_selection import train_test_split
import numpy as np


class LeafDetectionDataset(torch.utils.data.Dataset):
    def __init__(self, root, img_size, is_train=True, transforms=None):
        """
        Constructor of the LeafDetectionDataset
        :param root: the root folder of the dataset
        :param is_train: Whether to return the training or test set. Default: True.
        :param transforms: list of transformations to be applied to the data
        """
        self.root = root
        self.transforms = transforms
        self.resize = ResizeWithBBox(img_size)

        imgs = glob.glob(os.path.join(root, "*rgb.png"))
        self.img_files = imgs.copy()
        bboxes = glob.glob(os.path.join(root, "*bbox.csv"))
        imgs.sort()
        bboxes.sort()

        # Split the data into train and validation.
        x_train, x_test, y_train, y_test = train_test_split(imgs, bboxes, test_size=0.2, random_state=42)
        if is_train:
            imgs = x_train
            bboxes = y_train
        else:
            imgs = x_test
            bboxes = y_test

        # Read images and boxes and store them in a class attribute
        images_list = []
        bboxes_list = []
        for img_name, labels in zip(imgs, bboxes):
            img = cv2.imread(img_name)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(img)
            boxes = np.loadtxt(labels, delimiter=",")
            boxes = torch.tensor(boxes, dtype=torch.float32)
            if len(boxes.shape) == 1:
                boxes = boxes.unsqueeze(0)
            image, boxes = self.resize(image=img, boxes=boxes)
            boxes = torch.tensor(boxes, dtype=torch.float32)
            # TODO: convert the images from xyxy to cxcywh
            boxes = box_xyxy_to_cxcywh(boxes)
            images_list.append(image)
            bboxes_list.append(boxes)
        self.imgs = images_list
        self.bboxes = bboxes_list

    def __getitem__(self, idx):
        # Get image and boxes from list
        image = self.imgs[idx]
        bboxes = self.bboxes[idx]

        if self.transforms:  # Non geometric transforms
            image = self.transforms(image)

        # Create a torch tensor with zeros with represent the labels of the boxes
        # (There's only one class in this dataset)
        labels = torch.zeros((len(bboxes),), dtype=torch.int64)

        # TODO: add your code here
        # Since the number of bounding boxes (aka leaves) per image is different, we need to
        # create illegal boxes (with label=-1) so all images have the same number of boxes
        # and we can create batches
        # 50 should be a good max number
        illegal_needed = 50 - len(bboxes)
        illegal_boxes = torch.zeros((illegal_needed, 4), dtype=torch.float32) * -1
        illegal_labels = torch.ones((illegal_needed,), dtype=torch.int64) * -1

        return image, {
            "labels": torch.cat((labels, illegal_labels)),
            "boxes": torch.cat((bboxes, illegal_boxes), axis=0),
        }

    def __len__(self):
        # TODO: return the number of images in the dataset
        return len(self.imgs)

"""Now that we created our custom `Dataset` class, let's create an instance of it

#Develop CNN Models

We will try to make a models that achieve best result for object detection.

## Recomendations
This is a small dataset with roughly 100 images. The images present a low resolution and a big number of small objects. It's going to be a challenge for our simple Object Detection model. Don't be surprised if the average precision that you get oscilates between 0.2 and 0.3.

Things to try for better performance:
- Increase the resolution of the images. This also helps in the amount of objects that our detector can predict.
- Use data augmentations.
- Try different backbones/encoders.
- Freeze the backbone/encoder so its weights are not trained. This is useful when dealing with small datasets. You can use the following code for it:
    ```python
    backbone = models.resnet50(pretrained=pretrained)
    for param in backbone.parameters():
        param.requires_grad = False
    ```
- We have a lot of overlapping objects. Therefore, be aware of the Non-Maximum suppression threshold that you use for both calculating the AP and making predictions.
    - You can change the NMS threshold while calculating the AP during training like this: `ap = ap_calculator.calculate_map(model, nms_threshold=0.5)`
    - And for predictions like this: `boxes, top_class, scores = predict(model, img, n_classes=1, nms_threshold=0.3)`

## Original and augmented dataset
"""

def dataLoader(img_size, batch_size):
  augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

  train_dataset = LeafDetectionDataset(
      dataset_folder,
      img_size=(img_size, img_size),
      is_train=True,
      transforms=augs,
  )

  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)

  val_dataset = LeafDetectionDataset(
      dataset_folder,
      img_size=(img_size, img_size),
      is_train=False,
      transforms=augs,
  )

  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)

  return train_loader, val_loader, train_dataset, val_dataset

dataset_folder = 'leaf-dataset/detection'

img_size = 256
batch_size = 32

augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

train_dataset = LeafDetectionDataset(
    dataset_folder,
    img_size=(img_size, img_size),
    is_train=True,
    transforms=augs,
)

display_imgs_bbox = []

for i in range(10):
    img, target = train_dataset[i]
    img = ToPILImage()(img)
    img = plot_bbox(img, target["boxes"] * img_size)
    display_imgs_bbox.append(img)

# Plot two grids, one per list (don't forget the functions declared in the beginning of this notebook)
plot_grid(imgs=display_imgs_bbox, nrows=2, ncols=5)

"""As the training dataset is considered small compared to its corresponding test dataset, data augmentation might be needed to artificially generate new training images from the existing ones by applying various image transformation techniques.

Here, we define CustomTransform class to transform training images:
*   Randomly crop an area of 80% to 90% with the ratio of width to height randomly selected from between 1 and 2 of the original area
*   Randomly change the brightness, contrast, and saturation of the image to a value between 70% to 90%, and a contrast of 150% of the original images
"""

import random
class CustomTransform:
    """
    Applies custom transformations to an image and its bounding box.
    """
    def __init__(self, p=0.5):
        """
        Initializes the transform.
        Args:
            p: float, probability of applying the transform (default: 0.5)
        """
        self.p = p

    def __call__(self, img):
        """
        Applies the transform to an image and its corresponding bounding box.
        Args:
            image: PIL.Image object, containing the original image.
            box: bounding box coordinates in the format [x, y, width, height].
        Returns:
            image_transformed: Tensor, containing the transformed image.
            box_transformed: the bounding box coordinates for the transformed box.
        """
        # transform image to tensor, if no augmentation is applied
        img_transformed = transforms.ToTensor()(img)

        if random.random() < self.p:
            ###################
            # Get a few random components for transformations
            ###################

            # Apply a random scaling to the image, with a factor between 10-70% of the original

            ###################
            # Apply the transformations:
            ###################
            image_transform = transforms.Compose([
              torchvision.transforms.RandomResizedCrop((img_size, img_size), scale=(0.7, 0.9), ratio=(1, 2)),
              torchvision.transforms.ColorJitter(brightness=(0.8, 0.9), contrast=1.5,
                                                 saturation=(0.8, 0.9)),
              transforms.ToTensor()
            ])

            img_transformed = image_transform(img)

        return img_transformed

def dataLoaderAug(img_size, batch_size):
  augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

  train_dataset = LeafDetectionDataset(
      dataset_folder,
      img_size=(img_size, img_size),
      is_train=True,
      transforms=CustomTransform(p=1),
  )

  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)

  val_dataset = LeafDetectionDataset(
      dataset_folder,
      img_size=(img_size, img_size),
      is_train=False,
      transforms=augs,
  )

  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)

  return train_loader, val_loader, train_dataset, val_dataset

dataset_folder = 'leaf-dataset/detection'

img_size = 256
batch_size = 32

augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

train_augs_dataset = LeafDetectionDataset(
    dataset_folder,
    img_size=(img_size, img_size),
    is_train=True,
    transforms=CustomTransform(p=1),
)

display_imgs_bbox = []

for i in range(10):
    img, target = train_augs_dataset[i]
    img = ToPILImage()(img)
    img = plot_bbox(img, target["boxes"] * img_size)
    display_imgs_bbox.append(img)

# Plot two grids, one per list (don't forget the functions declared in the beginning of this notebook)
plot_grid(imgs=display_imgs_bbox, nrows=2, ncols=5)

"""### Multi-Scale Object Detector

Implement the simple object detector (multiScaleObjectDetector) that we learned in our object-detection sessions.
"""

class ObjectDetectorMultiScale(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScale, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.resnet18(pretrained=pretrained)
        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

def multiScaleObjectDetector(num_epochs, learning_rate, wd, PrintAP = False):
  device = d2l.try_gpu()
  model = ObjectDetectorMultiScale(n_classes=1, pretrained=True).to(device)
  matcher = HungarianMatcher()
  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=wd)
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
  print("total num of parameters in the model:", sum(p.numel() for p in model.parameters()))

  timer = d2l.Timer()
  animator = d2l.Animator(xlabel=f"epoch = {num_epochs}, lr = {learning_rate} and wd = {wd}", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

  ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

  for epoch in range(num_epochs):
      metric = d2l.Accumulator(4)
      model.train()
      for img, targets in train_loader:
          timer.start()

          img = img.to(device)

          # Remove illegal targets
          new_targets = []
          for i in range(targets["labels"].shape[0]):
              labels = targets["labels"][i]
              boxes = targets["boxes"][i]
              new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

          outputs = model(img)

          num_boxes = sum(len(t["labels"]) for t in new_targets)
          num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

          src_logits = outputs["pred_logits"]
          indices = matcher(outputs, new_targets)  # Run matcher
          idx = get_src_permutation_idx(indices)

          # Loss class
          target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
          target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
          target_classes[idx] = target_classes_o
          loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

          # loss boxes L1 and GIOU
          src_boxes = outputs["pred_boxes"][idx]
          target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

          loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
          loss_bbox = loss_bbox.sum() / num_boxes

          # TODO: sum the losses (1 line)
          loss = loss_ce + loss_bbox

          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          metric.add(
              loss_ce.cpu().detach().numpy(),
              loss_bbox.cpu().detach().numpy(),
              img.size()[0],
          )
      scheduler.step()
      cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
      ap = ap_calculator.calculate_map(model, nms_threshold=0.5)
      animator.add(epoch + 1, (cls_err, L1_error, ap))
  if (PrintAP == True):
      print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")
      print(f"Average Precision : " + str(round(ap,4)))

  return model

"""#### Experiment with original data"""

# Hyperparameters
batch_size = 32
img_size = 256
learning_rate = 1e-4
num_epochs = 35
wd = 1e-4

# Use the original dataset, thus call the dataLoader function
train_loader, val_loader, train_dataset, val_dataset = dataLoader(img_size, batch_size)

# Implement it to the model
modelOriginalData = multiScaleObjectDetector(num_epochs, learning_rate, wd, PrintAP = True)

img, _ = val_dataset[random.randint(0, len(val_dataset) - 1)]

boxes, top_class, scores = predict(modelOriginalData, img, n_classes=1, nms_threshold=0.3)

plot_img = plot_bbox(ToPILImage()(img), boxes)
plt.imshow(plot_img)

"""#### Experiment with augmented data"""

# Hyperparameters
batch_size = 32
img_size = 256
learning_rate = 1e-4
num_epochs = 35
wd = 1e-4

# Use the augmented dataset, thus call the dataLoaderAug function
train_loader, val_loader, train_dataset, val_dataset = dataLoaderAug(img_size, batch_size)

# Implement it to the model
modelAugData = multiScaleObjectDetector(num_epochs, learning_rate, wd, PrintAP = True)

img, _ = val_dataset[random.randint(0, len(val_dataset) - 1)]

boxes, top_class, scores = predict(modelAugData, img, n_classes=1, nms_threshold=0.3)

plot_img = plot_bbox(ToPILImage()(img), boxes)
plt.imshow(plot_img)

"""#### Experiment with image size

In this section we want to experiments with variations of image sizes, learning rate, epochs and weight decay. We still using original data without the augmented.
"""

# Hyperparameters
batch_size = 32
learning_rate = 1e-4
num_epochs = 35
wd = 1e-4

imgsizeArray = [128, 256, 384]

models_img_size_variation = []

# Use the original dataset, thus call the dataLoader function
train_loader, val_loader, train_dataset, val_dataset = dataLoader(img_size, batch_size)

for img_size in imgsizeArray:
    model_img_size = multiScaleObjectDetector(num_epochs, learning_rate, wd)
    models_img_size_variation.append(model_img_size)

"""#### Experiment with learning rate

In this code we will make a for loop from an array list with different Learning rate
"""

# Hyperparameters
batch_size = 32
img_size = 256
num_epochs = 35
wd = 1e-4

learningRateArray=[1e-2, 1e-3, 1e-4]

models_learning_rate_variation = []

# Use the original dataset, thus call the dataLoader function
train_loader, val_loader, train_dataset, val_dataset = dataLoader(img_size, batch_size)

for learning_rate in learningRateArray:
    model_learning_rate = multiScaleObjectDetector(num_epochs, learning_rate, wd)
    models_learning_rate_variation.append(model_learning_rate)

"""#### Experiment with epochs"""

# Hyperparameters
batch_size = 32
img_size = 256
learning_rate = 1e-4
wd = 1e-4

epochsArray=[20, 35, 50] #1e-4

models_epochs_variation = []

# Use the original dataset, thus call the dataLoader function
train_loader, val_loader, train_dataset, val_dataset = dataLoader(img_size, batch_size)

for num_epochs in epochsArray:
    model_epoch = multiScaleObjectDetector(num_epochs, learning_rate, wd)
    models_epochs_variation.append(model_epoch)

"""#### Experiment with weight decay"""

# Hyperparameters
batch_size = 32
img_size = 256
learning_rate = 1e-4
num_epochs = 35

wdArray=[1e-2, 1e-3, 1e-4]

models_wd_variation = []

# Use the original dataset, thus call the dataLoader function
train_loader, val_loader, train_dataset, val_dataset = dataLoader(img_size, batch_size)

for wd in wdArray:
    model_epoch = multiScaleObjectDetector(num_epochs, learning_rate, wd)
    models_wd_variation.append(model_epoch)

"""### Different encoders

After experiment with different original data and original, also with image size, bath size, learning rate, number of epochs and weight decay. We choose this hyperparameters as the best.

We experiment with 3 different backbones/encoders, such as resnet18, resnet34 and VGG16.
"""

# Hyperparameters
batch_size = 32
img_size = 128
learning_rate = 1e-4
num_epochs = 35
wd = 1e-4

# Use the augmented dataset, thus call the dataLoaderAug function
train_loader, val_loader, train_dataset, val_dataset = dataLoaderAug(img_size, batch_size)

"""#### ResNet18"""

class ObjectDetectorMultiScaleResNet18(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleResNet18, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.resnet18(pretrained=pretrained)
        #backbone.fc = nn.Linear(in_features = 2, out_features = 2048)
        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelResNet18 = ObjectDetectorMultiScaleResNet18(n_classes=1, pretrained=True)
summary(modelResNet18, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelResNet18 = ObjectDetectorMultiScaleResNet18(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelResNet18.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelResNet18.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelResNet18.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelResNet18(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_ResNet18 = ap_calculator.calculate_map(modelResNet18, nms_threshold=0.5)
    animator.add(epoch + 1, (cls_err, L1_error, ap_ResNet18))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### ResNet34"""

class ObjectDetectorMultiScaleResNet34(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleResNet34, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.resnet34(pretrained=pretrained)
        #backbone.fc = nn.Linear(in_features = 2, out_features = 2048)
        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelResNet34 = ObjectDetectorMultiScaleResNet34(n_classes=1, pretrained=True)
summary(modelResNet34, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelResNet34 = ObjectDetectorMultiScaleResNet34(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelResNet34.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelResNet34.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelResNet34.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelResNet34(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_ResNet34 = ap_calculator.calculate_map(modelResNet34, nms_threshold=0.5)
    animator.add(epoch + 1, (cls_err, L1_error, ap_ResNet34))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### VGG16"""

class ObjectDetectorMultiScaleVGG16(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleVGG16, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.vgg16(pretrained=pretrained)
        #backbone.fc = nn.Linear(in_features = 2, out_features = 2048)
        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelVGG16 = ObjectDetectorMultiScaleVGG16(n_classes=1, pretrained=True)
summary(modelVGG16, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelVGG16 = ObjectDetectorMultiScaleVGG16(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelVGG16.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelVGG16.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelVGG16.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelVGG16(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_VGG16 = ap_calculator.calculate_map(modelVGG16, nms_threshold=0.3)
    animator.add(epoch + 1, (cls_err, L1_error, ap_VGG16))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### Results comparison between encoders"""

# img, _ = val_dataset[random.randint(0, len(val_dataset) - 1)]
img, _ = val_dataset[13]
nms_threshold_input = 0.3

# Change the NMS threshold for predictions
boxesResNet18, top_classResNet18, scoresResNet18 = predict(modelResNet18, img, n_classes=1, nms_threshold=nms_threshold_input)
boxesResNet34, top_classResNet34, scoresResNet34 = predict(modelResNet34, img, n_classes=1, nms_threshold=nms_threshold_input)
boxesVGG16, top_classVGG16, scoresVGG16 = predict(modelVGG16, img, n_classes=1, nms_threshold=nms_threshold_input)

plot_imgResNet18 = plot_bbox(ToPILImage()(img), boxesResNet18)
plot_imgResNet34 = plot_bbox(ToPILImage()(img), boxesResNet34)
plot_imgVGG16 = plot_bbox(ToPILImage()(img), boxesVGG16)

print(f"Average Precison of ResNet18 : " + str(round(ap_ResNet18,4)))
print(f"Average Precison of ResNet34 : " + str(round(ap_ResNet34,4)))
print(f"Average Precison of VGG16    : " + str(round(ap_VGG16,4)))

# Plot the three images side by side
fig, axs = plt.subplots(1, 3, figsize=(10,5))

plt.subplot(1, 3, 1)
plt.imshow(plot_imgResNet18)
plt.title('ResNet18')

plt.subplot(1, 3, 2)
plt.imshow(plot_imgResNet34)
plt.title('ResNet34')

plt.subplot(1, 3, 3)
plt.imshow(plot_imgResNet34)
plt.title('VGG16')

plt.show()

"""### Different encoders with freezing weights (FW)

- We try to implement freeze the backbone/encoder so its weights are not trained. This is useful when dealing with small datasets. We can use the following code from recommendation for it:
    ```python
    backbone = models.resnet50(pretrained=pretrained)
    for param in backbone.parameters():
        param.requires_grad = False
    ```
"""

# Hyperparameters
batch_size = 32
img_size = 128
learning_rate = 1e-4
num_epochs = 35
wd = 1e-4

# Use the augmented dataset, thus call the dataLoaderAug function
train_loader, val_loader, train_dataset, val_dataset = dataLoaderAug(img_size, batch_size)

"""#### ResNet18FW"""

class ObjectDetectorMultiScaleResNet18FW(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleResNet18FW, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.resnet18(pretrained=pretrained)
        for param in backbone.parameters():
            param.requires_grad = False

        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelResNet18FW = ObjectDetectorMultiScaleResNet18FW(n_classes=1, pretrained=True)
summary(modelResNet18FW, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelResNet18FW = ObjectDetectorMultiScaleResNet18FW(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelResNet18FW.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelResNet18FW.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelResNet18FW.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelResNet18FW(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_ResNet18_FW = ap_calculator.calculate_map(modelResNet18FW, nms_threshold=0.5)
    animator.add(epoch + 1, (cls_err, L1_error, ap_ResNet18_FW))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### ResNet34FW"""

class ObjectDetectorMultiScaleResNet34FW(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleResNet34FW, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.resnet34(pretrained=pretrained)
        for param in backbone.parameters():
            param.requires_grad = False

        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelResNet34FW = ObjectDetectorMultiScaleResNet34FW(n_classes=1, pretrained=True)
summary(modelResNet34FW, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelResNet34FW = ObjectDetectorMultiScaleResNet34FW(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelResNet34FW.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelResNet34FW.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelResNet34FW.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelResNet34FW(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_ResNet34_FW = ap_calculator.calculate_map(modelResNet34FW, nms_threshold=0.5)
    animator.add(epoch + 1, (cls_err, L1_error, ap_ResNet34_FW))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### VGG16FW"""

class ObjectDetectorMultiScaleVGG16FW(nn.Module):
    def __init__(self, n_classes, pretrained=True):
        super(ObjectDetectorMultiScaleVGG16FW, self).__init__()

        # We add the background class
        self.n_classes = n_classes + 1

        # Backbone
        backbone = models.vgg16(pretrained=pretrained)
        #backbone.fc = nn.Linear(in_features = 2, out_features = 2048)
        self.backbone = nn.Sequential(*list(backbone.children())[:-3])

        # Layer 1
        self.block_1 = nn.Sequential(*list(backbone.children())[-3])

        # Layer 2
        self.block_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(2, 2), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Block 3
        self.block_3 =nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Dropout(p=0.2),
        )

        # Box Predictor
        self.box_predictor = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

        # Box Predictor
        self.box_predictor_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.Sigmoid(),
        )

        # Classifier
        self.classifier_2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(negative_slope=0.1),
            nn.Conv2d(in_channels=512, out_channels=self.n_classes, kernel_size=(3, 3), stride=(1, 1), padding=1),
        )

    def forward(self, x):
        bs, nc, w, h = x.size()
        features = self.backbone(x)
        y_ = self.block_1(features)
        y = self.block_2(y_)

        box_preds = self.box_predictor(y)
        box_preds = box_preds.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds = self.classifier(y)
        cls_preds = cls_preds.reshape(bs, self.n_classes, -1).permute(0, 2, 1)

        y2 = self.block_3(y_)
        box_preds_2 = self.box_predictor_2(y2)
        box_preds_2 = box_preds_2.reshape(bs, 4, -1).permute(0, 2, 1)
        cls_preds_2 = self.classifier_2(y2)
        cls_preds_2 = cls_preds_2.reshape(bs, self.n_classes, -1).permute(0, 2, 1)
        return {
            "pred_logits": torch.cat((cls_preds, cls_preds_2), dim=1),
            "pred_boxes": torch.cat((box_preds, box_preds_2), dim=1),
        }

modelVGG16FW = ObjectDetectorMultiScaleVGG16FW(n_classes=1, pretrained=True)
summary(modelVGG16FW, (3, 256, 256), device="cpu")

# TODO: add your training code here

device = d2l.try_gpu()
modelVGG16FW = ObjectDetectorMultiScaleVGG16FW(n_classes=1, pretrained=True).to(device)
matcher = HungarianMatcher()
optimizer = torch.optim.AdamW(modelVGG16FW.parameters(), lr=learning_rate, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
print("total num of parameters in the modelResNet18:", sum(p.numel() for p in modelVGG16FW.parameters()))

timer = d2l.Timer()
animator = d2l.Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["class error", "L1 error", "val AP"])

ap_calculator = APCalculator(val_loader, n_classes=1, iou_threshold=0.5)

for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)
    modelVGG16FW.train()
    for img, targets in train_loader:
        timer.start()

        img = img.to(device)

        # Remove illegal targets
        new_targets = []
        for i in range(targets["labels"].shape[0]):
            labels = targets["labels"][i]
            boxes = targets["boxes"][i]
            new_targets.append({"labels": labels[labels != -1].to(device), "boxes": boxes[labels != -1].to(device)})

        outputs = modelVGG16FW(img)

        num_boxes = sum(len(t["labels"]) for t in new_targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)

        src_logits = outputs["pred_logits"]
        indices = matcher(outputs, new_targets)  # Run matcher
        idx = get_src_permutation_idx(indices)

        # Loss class
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(new_targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], 1, dtype=torch.int64, device=device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes)

        # loss boxes L1 and GIOU
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")
        loss_bbox = loss_bbox.sum() / num_boxes

        # TODO: sum the losses (1 line)
        loss = loss_ce + loss_bbox

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        metric.add(
            loss_ce.cpu().detach().numpy(),
            loss_bbox.cpu().detach().numpy(),
            img.size()[0],
        )
    scheduler.step()
    cls_err, L1_error = metric[0] / metric[2], metric[1] / metric[2]
    # ap = ap_calculator.calculate_map(model)
    # Change the NMS threshold while calculating the AP during training
    ap_VGG16_FW = ap_calculator.calculate_map(modelVGG16FW, nms_threshold=0.5)
    animator.add(epoch + 1, (cls_err, L1_error, ap_VGG16_FW))
print(f"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on " f"{str(device)}")

"""#### Results comparison between encoders FW"""

# img, _ = val_dataset[random.randint(0, len(val_dataset) - 1)]
img, _ = val_dataset[13]
nms_threshold_input = 0.3

# Change the NMS threshold for predictions
boxesResNet18FW, top_classResNet18FW, scoresResNet18FW = predict(modelResNet18FW, img, n_classes=1, nms_threshold=nms_threshold_input)
boxesResNet34FW, top_classResNet34FW, scoresResNet34FW = predict(modelResNet34FW, img, n_classes=1, nms_threshold=nms_threshold_input)
boxesVGG16FW, top_classVGG16FW, scoresVGG16FW = predict(modelVGG16FW, img, n_classes=1, nms_threshold=nms_threshold_input)

plot_imgResNet18 = plot_bbox(ToPILImage()(img), boxesResNet18FW)
plot_imgResNet34 = plot_bbox(ToPILImage()(img), boxesResNet34FW)
plot_imgVGG16 = plot_bbox(ToPILImage()(img), boxesVGG16FW)

print(f"Average Precison of ResNet18FW  : " + str(round(ap_ResNet18_FW,4)))
print(f"Average Precison of ResNet34FW : " + str(round(ap_ResNet34_FW,4)))
print(f"Average Precison of VGG16FW     : " + str(round(ap_VGG16_FW,4)))

# Plot the three images side by side
fig, axs = plt.subplots(1, 3, figsize=(10,5))

plt.subplot(1, 3, 1)
plt.imshow(plot_imgResNet18)
plt.title('ResNet18')

plt.subplot(1, 3, 2)
plt.imshow(plot_imgResNet34)
plt.title('ResNet34')

plt.subplot(1, 3, 3)
plt.imshow(plot_imgResNet34)
plt.title('VGG16')

plt.show()