# Leaf Counting and Detection
Deep Learning Course (GRS-4806)  
Wageningen University
![image](https://github.com/user-attachments/assets/dc47b632-9e38-41ce-941c-2f0d9f36f82a)

## Introduction
Plant phenotype is a set of observable characteristics resulting from interactions between gene expression and the environment. Accurate and efficient monitoring of plant phenotype is essential for intelligent production in plant cultivation, which leads to better decision-making resulting in a higher yield. Traditional plant monitoring still uses manual measurement to analyze phenotypes, which are labor-intensive, time-consuming, and biased toward the observer. A better approach is to use image-based plant phenotyping using deep learning that allows distant observation and reduces the effects of manual interference. Convolutional neural network (CNN)-based methods have commonly been applied to plant phenotyping. Related works are from [1] that used CNN for leaf counting, and [2][3] that used CNN for leaf detection.
  
In this project, we will use datasets from Leaf Counting Challenge (LCC) and Leaf Segmentation Challenge (LSC) as input to do the two tasks, leaf counting and leaf detection. The architectures that we use are ObjectDetectorMultiScale, AlexNet, pre-trained ResNet18, pre-trained ResNet34, pre-trained ResNet50, and VGG16 to be compared and evaluate their performances. We also implemented some experiments with data augmentation, a variation of hyperparameters, and Freezing Weights (FW), to make the architectures more robust and get the best performance for both tasks. Lastly, we compare the performance with existing results from papers.

## Methods
### Dataset
Our source of datasets is ‘Ara2012’ and ‘Ara2013-Canon’ sets from the Leaf Segmentation Challenge (LSC) and Leaf Counting Challenge (LCC) that contain top-down images of Arabidopsis plants and ground-truth annotations. One of the main challenges is we don’t have enough data to train on. The original dataset only contains roughly 150 images for the training set and 50 for the test set. We use the data augmentation technique to increase our dataset artificially to avoid overfitting. The augmented dataset is made by: 
-	Randomly cropping an area of 70% to 90% of the original size, and the ratio of width to height of the region is randomly selected from between 1 and 2.
-	Randomly change the brightness and saturation of the image to a value between 80% to 90% with a contrast of 150% of the original images.
-	A horizontal flip is applied with a 50% probability.

### Task 1: Leaf Counting
For the leaf counting task, we developed a CNN regression network using convolutional layers, pooling layers, and fully connected layers in the end and trained it on the training set. We use three different networks to compare: 
-	AlexNet using hyperparameters batch size = 256, image size = 128, learning rate = 10-4, and epochs = 35 
-	Fine-tuning the pre-trained ResNet18 using hyperparameters batch size = 256, image size = 128, learning rate = 5x10-5, and epochs = 50
-	Fine-tuning the pre-trained ResNet50 using hyperparameters batch size = 256, image size = 128, learning rate = 5x10-5, and epochs = 50

We also define Mean Squared Error (MSE) as the loss function and Adam as the optimizer. As this is a regression task, the performance of these models is evaluated by calculating its Pearson correlation coefficient (r) on the test set. The closer the value is to 1 or -1, the closer the network predictions are to the ground truth.

### Task 2: Leaf Detection
For the leaf detection task, we developed a CNN based trained it on the training set. We use four different networks to compare: 
o	A simple object detector called ObjectDetectorMultiScale using hyperparameters batch size = 32, image size = 256, learning rate = 10-4, epochs = 35, and weight decay = 10-4. We used different experiments for this network:
-	Using original data.
-	Using augmented data.
-	Using a variation of image size = [128, 256, 384].
-	Using a variation of learning rate = [10-2, 10-3, 10-4].
-	Using a variation of epochs = [20, 35, 50].
-	Using a variation of weight decay = [10-2, 10-3, 10-4].
o	ResNet18 using hyperparameters batch size = 32, image size = 128, learning rate = 10-4, epochs = 35, and weight decay = 10-4. We implement the model with and without freezing weights (FW).
o	ResNet34 using hyperparameters batch size = 32, image size = 128, learning rate = 10-4, epochs = 35, and weight decay = 10-4. We implement the model with and without freezing weights (FW).
o	VGG16 using hyperparameters batch size = 32, image size = 128, learning rate = 10-4, epochs = 35, and weight decay = 10-4. We implement the model with and without freezing weights (FW).
We also define Cross Entropy as the loss function and Adam as the optimizer. The performance of these models is evaluated by calculating their Average Precision (AP) on the test set. The closer the value is to 1, the closer the network predictions are to the ground truth. As we have a lot of overlapping objects, we change the non-maximum suppression threshold for calculating the AP to 0.5 and making predictions to 0.3.

## Results and Discussions
### Task 1: Leaf Counting
The training of the three models (AlexNet, pre-trained ResNet18, and pre-trained ResNet50) on the dataset is implemented in Python 3.6 and PyTorch as the back-end using Google Colab. As mentioned in the methods section, we use the data augmentation technique to mitigate the small amount of our train data to avoid overfitting. Figure 1 shows the differences between original and augmented images. For experiment purposes, we trained the original and augmented datasets separately to all models, which took us between 0.5 and 1 hour to converge, as training graphs shown in Figure 2. The Pearson correlation coefficient (r) then evaluates their performance.
Results from Table 1 demonstrate that AlexNet using the augmented dataset has given the best performance over other models with r = 0.76, an improvement from AlexNet when using the original dataset with r = 0.74. The alleged boost in performance comes from data augmentation applications. An experiment conducted by Gomes and Zheng using the same LSC and LCC datasets for leaf counting tasks showed a 2% improvement in performance metrics after data augmentation [1]. They also stated that data augmentation is significant for regularizing the models trained on limited datasets by generalizing out-of-sample images, which explains why there is an increase in performance.
However, based on our observation, data augmentation is not a one-size-fits-all solution to increase performance. Results from Table 1 demonstrate that ResNet50 using the original dataset, performs better than the augmented one. According to Gomes and Zheng, data augmentations are effective but can be too domain-specific [1]. The original dataset could fit better than the augmented dataset based on the characteristic of the ResNet50 model combined with our small training dataset. But if we analyze more, the ResNet50 using the augmented dataset converges faster, as shown in Figure 2. If the performance metrics also include the inference time, using the augmented data is better than using the original one. Also, the current results may change if we add new training images, alter the data augmentation technique, or change the hyperparameters.
Furthermore, results from Table 1 demonstrate that AlexNet performance is superior to pre-training ResNet18 and ResNet50. Unlike AlexNet, we use ResNet as a pre-trained model using fine-tuning from the ImageNet dataset to transfer learning with our training dataset. The pre-trained model can be an alternative solution in combating a small training dataset, as it already has well-balanced weights from an extensive dataset. However, according to Poojary et al., the main limitation of pre-trained models is they are built with standard models, so making changes to this model may severely affect the performance of the models [4]. We modify the last layer of the ResNet model by flattening it to solve a regression task, and this could lead to lower performance which explains why AlexNet is better in this case.
In conclusion, we use several CNN models to do a leaf counting task and find that the best performance model is AlexNet using data augmentation with r = 0.76. When challenging small train datasets, data augmentation has proven can increase performance considerably. However, using different data augmentation techniques may change the results. AlexNet also performs better than pre-training ResNet, as pre-trained models perform less when we modify the model to solve this specific regression task. Future research is necessary to improve model performance by modifying the data augmentation, using other models, and increasing the size and variability training dataset with new images.

### Task 2: Leaf Detection
The training of the various models and their experiments (ObjectDetectorMultiScale, pre-trained ResNet18, pre-trained ResNet34, pre-trained VGG16) on the dataset is implemented in Python 3.6 and PyTorch as the back-end using Google Colab. The learning process for whole process took about between 1 and 1.5 hour to converge.
By using augmented data we can increase the AP with small datasets, The authors of [2] proposed a data augmentation method, preserving the photorealistic appearance of plant leaves and we implemented it on the methods. 
Upon analyzing the data presented in both Figure 3, it is evident that data augmentation has been applied to the original dataset. The deep learning model, ObjectDetectorMultiScale, has been trained using both the original and augmented data, and the results have been recorded in Figure 4 and Table 3. The Average Precision (AP) then evaluates their performance. It is noteworthy that the performance of the model has improved with the use of augmented data, highlighting the importance of data augmentation in deep learning applications. Specifically, when using the original dataset without augmentation, the model achieved a AP score of 0.33, whereas with augmented data, its performance increased to 0.37, representing an improvement of 0.04 or 4%. The augmentation techniques used in this study were limited to random cropping and adjusting brightness.
In our experiment on leaf detection using deep learning, we also tested various hyperparameters and the results in Table 2. We found that using the original dataset without augmentation and an image size of 128 pixels yielded the highest average precision of 0.32. Additionally, a learning rate of 0.0001 resulted in an AP of 0.33, while training for 35 epochs gave an AP of 0.34. Lastly, a weight decay of 0.0001 was found to be optimal, resulting in an AP of 0.30. Based on these results, we have chosen these hyperparameters for use in architectures that involve encoders and Freezing Weights (FW). 
The results presented in Table 3 show the average precision scores for different pre-trained deep learning architectures with augmented data. Among the various encoder architectures, ResNet34 demonstrated the best performance, achieving an average precision score of 0.38, followed by ResNet18 with a score of 0.31, and VGG16 with a score of 0.30. Additionally, FW for ResNet18 and ResNet34 resulted in AP scores of 0.26 and 0.23, respectively. However, VGG16 with FW outperformed the other architectures with a score of 0.38, which is the highest average precision score achieved in this experiment and most reliable. These findings suggest that VGG16 with FW could be a use for object detection tasks that have small datasets. 
In terms of performance on small leaf datasets, it is possible that ResNet18 and ResNet34 might outperform YOLO, as they are designed for image classification tasks and have a lower computational cost compared to YOLO. However, YOLO has the advantage of being specifically designed for object detection tasks and may have better performance in detecting small objects, such as leaves [3].
Our experiments faced several limitations that affected the accuracy of our models. One significant limitation was the small size of our dataset, which may have limited the ability of our models to learn complex features and patterns. As a result, we may not have achieved the highest possible accuracy with the VGG16FW model. Additionally, we had limited GPU resources, which prevented us from deploying more complex models with higher parameters such as ResNet50. 
In conclusion, we use several CNN variation models to do a leaf detection task and find that the best performance model is VGG16FW using data augmentation with AP = 0.38. Despite the limitations our project demonstrated the effectiveness of Non-maximum Suppression (NMS) in improving the accuracy of object detection models. By using a threshold of 0.5, NMS was able to differentiate overlapping objects and achieve significant improvement in our models. Tuning the NMS threshold can also become an important parameter to consider when training object detection models. In addition, using more data augmentation techniques can increase the dataset variations and improve the training process. Going forward, increasing the size of the dataset can potentially improve the accuracy of the model.

## References

## Figures and Tables
